{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fertig\n"
     ]
    }
   ],
   "source": [
    "## Importieren der erforderlichen Bibliotheken\n",
    "# Pyhton Version 3.11.1\n",
    "# Selenium Version 4.9.1\n",
    "# webdriver_manager Version 3.8.6\n",
    "# pandas Version  2.0.1\n",
    "# datetime Version 5.1\n",
    "# sqlalchemy Version 2.0.13\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "# import pandas as pd\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import tools\n",
    "\n",
    "tabelle_rohdaten=\"jobs.rohdaten\"\n",
    "scraper_name = \"stepstone\"\n",
    "\n",
    "\n",
    "## Der Scraper\n",
    "def stepstone_scraper(jobtitel, suchort='Deutschland'):\n",
    "    '''\n",
    "    Der eigentliche Scraper für die Seite\n",
    "    \n",
    "    :param jobtitel: Der Titel des Jobs, nach dem gesucht werden soll.\n",
    "    :param suchort: Der Ort, an dem nach Jobs gesucht werden soll. Standardwert ist 'Deutschland'.\n",
    "    '''\n",
    "    \n",
    "    tools.schreibe_log_file(scraper_name, f'suche nach {jobtitel}')\n",
    "    \n",
    "    con = tools.connect_db()\n",
    "\n",
    "    # ChromeOptions erstellen\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    #Headless-Modus aktivieren\n",
    "    #chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "    # Browserfenster öffnen nach Einstellung in den Optionen\n",
    "    try:\n",
    "        driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=chrome_options)\n",
    "        tools.wartezeit(1, 1) # Wartezeit das Browser geladen ist\n",
    "        tools.schreibe_log_file(scraper_name, 'Browser geoeffnet')\n",
    "    except:\n",
    "        tools.schreibe_log_file(scraper_name, 'Browser konnte nicht geoeffnet werden')\n",
    "        tools.schreibe_e_mail(scraper_name, f'Browser konnte nicht geoeffnet werden: -- {datetime.datetime.now().strftime(\"%d.%m.%Y, %H:%M:%S\")} --')\n",
    "\n",
    "    # Webseite aufrufen\n",
    "    driver.get(\"https://www.stepstone.de/\")\n",
    "\n",
    "    # Fenster Maximieren\n",
    "    driver.maximize_window() # evtl. im Headless-Modus nicht machbar\n",
    "\n",
    "    # Cookies akzeptieren\n",
    "    time.sleep(2)\n",
    "    driver.find_element(By.XPATH, '//*[@id=\"ccmgt_explicit_accept\"]').click()\n",
    "    tools.wartezeit(0.5,1)\n",
    "    \n",
    "    ## Suchfelder finden, anwählen und befüllen\n",
    "    # Suchfeld Jobbezeichnung -> finden und befüllen\n",
    "    suchfeld_jobtitel=driver.find_element(By.XPATH,'//*[@id=\"stepstone-autocomplete-162\"]')\n",
    "    suchfeld_jobtitel.send_keys(jobtitel)\n",
    "    tools.wartezeit(0.5, 1)\n",
    "\n",
    "    # Suchfeld Ort -> finden und befüllen\n",
    "    suchfeld_ort=driver.find_element(By.XPATH,'//*[@id=\"stepstone-form-element-173-input\"]')\n",
    "    suchfeld_ort.send_keys(suchort)\n",
    "    tools.wartezeit(0.5, 1)\n",
    "    \n",
    "    # Anfrage mit Enter/Return abschicken\n",
    "    suchfeld_ort.send_keys(Keys.RETURN)\n",
    "    tools.wartezeit(0.5, 1)\n",
    "    \n",
    "    ## nach Datum sortieren\n",
    "    # erst Filterfeld dann Datum klicken\n",
    "    driver.find_element(By.CLASS_NAME, 'res-1vztcyh').click()\n",
    "    tools.wartezeit(0.5, 1)\n",
    "    driver.find_element(By.XPATH, '//*[@id=\"date\"]/span/span[2]').click()\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Leere Liste erstellen für die Links\n",
    "    link_liste_scraped=[]\n",
    "    \n",
    "    ## Namen für verschiedene XPATH, CLASS, CSS, etc.\n",
    "    # name für den Container der alle Stellenanzeigen enthält\n",
    "    class_name_anzeige_link='res-kyg8or'\n",
    "\n",
    "    # name für den Container für alle URLs der einzelnen Stellenanzeigen\n",
    "    # da es meherer Namen gibt, mit Schleife herausfinde welche es gibt\n",
    "    for class_name in ['res-3yv1ty','res-2cltag']:\n",
    "        if driver.find_elements(By.CLASS_NAME, class_name) != []:\n",
    "            class_name_url_link=class_name\n",
    "    \n",
    "    ## Aus den Stellenanzeigen die Links zu den Stellenanzeigen finden und in eine Liste packen\n",
    "    # alle Stellenanzeigen auf der seite finden\n",
    "    anzeigen = driver.find_elements(By.CLASS_NAME, class_name_anzeige_link)\n",
    "\n",
    "    # in jeder Stellenazige die URL heraussuchen\n",
    "    for anzeige in anzeigen:\n",
    "        try:\n",
    "            for link in anzeige.find_elements(By.CLASS_NAME, class_name_url_link):\n",
    "                link_liste_scraped.append(link.get_attribute('href'))\n",
    "        except:\n",
    "            tools.schreibe_log_file(scraper_name, f'Link in Anzeige auf erster Seite nicht gefunden')\n",
    "            continue\n",
    "            \n",
    "    # weitere Seiten aufrufen\n",
    "    for i in range(9):\n",
    "        \n",
    "        # Link zur nächsten seite finden und aufrufen\n",
    "        try:\n",
    "            driver.get(driver.find_elements(By.CLASS_NAME, 'res-1w7ajks')[1].get_attribute('href'))\n",
    "        except:\n",
    "            tools.schreibe_log_file(scraper_name, 'Class-Name nicht gefunden für nächsten seite der Stellenanzeigen')\n",
    "            pass\n",
    "        #Warten auf den Aufbau der Seite\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # alle Stellenanzeigen auf der seite finden\n",
    "        anzeigen = driver.find_elements(By.CLASS_NAME, class_name_anzeige_link)\n",
    "\n",
    "        # in jeder Stellenazige die URL heraussuchen\n",
    "        for anzeige in anzeigen:\n",
    "            for link in anzeige.find_elements(By.CLASS_NAME, class_name_url_link):\n",
    "                link_liste_scraped.append(link.get_attribute('href'))\n",
    "                \n",
    "    # überprüfen ob link_liste_scraped leer ist da evtl. die Class name geändert wurde\n",
    "    if len(link_liste_scraped) == 0:\n",
    "        tools.schreibe_log_file(scraper_name, f'Keine Links auf gefunden: Jobtitel => {jobtitel}')\n",
    "        tools.schreibe_e_mail(scraper_name, f'Keine Links auf Webseite gefunden: Jobtitel => {jobtitel}\\n\\n Hinweis: Classenname überprüfen für alle Stellenanzeigen!')\n",
    "        return\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # Duplikate entfernen\n",
    "    link_liste_scraped_ohne_duplikate = list(set(link_liste_scraped))\n",
    "    tools.schreibe_log_file(scraper_name, f'Es wurden {len(link_liste_scraped)-len(link_liste_scraped_ohne_duplikate)} Duplikate in der \"Link Liste\" entfernt')\n",
    "    tools.schreibe_log_file(scraper_name, \"link liste erstellt\")\n",
    "    \n",
    "    # Lese die bestehenden URLs aus der Datenbank\n",
    "    existing_urls = pd.read_sql_query(f\"SELECT url FROM {tabelle_rohdaten}\", con)\n",
    "    tools.schreibe_log_file(scraper_name, f'{len(existing_urls)} in der datenbank bereits vorhanden')\n",
    "\n",
    "    # Filtere den DataFrame, um nur neue URLs zu behalten\n",
    "    link_liste = [url for url in link_liste_scraped_ohne_duplikate if url not in existing_urls['url'].values]\n",
    "    \n",
    "    # Listen für für den DataFrame\n",
    "    seiten_inhalt_html_liste = []\n",
    "    seiten_inhalt_liste = []\n",
    "    URL_Liste = []\n",
    "    Datum_Liste = []\n",
    "\n",
    "    ## Scrapen\n",
    "    try:\n",
    "        tools.schreibe_log_file(scraper_name, 'Abfrage begonnen')\n",
    "        for link in link_liste:\n",
    "            \n",
    "            # Aufrufen der Seite\n",
    "            try:\n",
    "                driver.get(link)\n",
    "                tools.wartezeit(0.5, 2)\n",
    "            except:\n",
    "                tools.schreibe_log_file(scraper_name, f'Fehler beim aufrufen der URL: {link}')\n",
    "            \n",
    "            # Prüfen ob ein bestimmter Text auf der Seite steht, der auf die nicht mehr existierende Anzeige hindeutet\n",
    "            try:\n",
    "                stellenanzeige_pruefen=driver.find_element(By.CLASS_NAME, 'listing-content-provider-1qtvd67').text\n",
    "            except:\n",
    "                stellenanzeige_pruefen=\"\"\n",
    "            \n",
    "            # Prüfen ob Stellenanzeige existiert oder nicht \n",
    "            if stellenanzeige_pruefen != 'Diese Stellenanzeige ist nicht mehr verfügbar.':\n",
    "                            \n",
    "                ## Grunddaten scrapen\n",
    "                seiten_inhalt_html_liste.append(driver.find_element(By.CLASS_NAME, 'reb-main').get_attribute('innerHTML'))\n",
    "                seiten_inhalt_liste.append(driver.find_element(By.CLASS_NAME, 'reb-main').text)\n",
    "                    \n",
    "                # URL der Stellenanzeige\n",
    "                URL_Liste.append(link)\n",
    "                \n",
    "                # Zeitstempel\n",
    "                Datum_Liste.append(datetime.datetime.now())\n",
    "                \n",
    "                tools.schreibe_log_file(scraper_name, f'Daten wurden gezogen: {link}')\n",
    "            else:\n",
    "                tools.schreibe_log_file(scraper_name, f'Stellenanzeige nicht verfügbar:  {link}')\n",
    "    except:\n",
    "        tools.schreibe_log_file(scraper_name, \"Abfrage abgebrochen !!!\")\n",
    "        tools.schreibe_e_mail(scraper_name, f\"Die Abfrage wurde abgebrochen. -- {datetime.datetime.now()} --\")\n",
    "    tools.schreibe_log_file(scraper_name, 'Abfrage beendet')\n",
    "    \n",
    "    # Schließen des Browsers\n",
    "    driver.quit()\n",
    "    tools.schreibe_log_file(scraper_name, 'Browser geschlossen')\n",
    "    time.sleep(3)\n",
    "        \n",
    "    # Prüfen ob die Listen gleich lang sind\n",
    "    if len(set(map(len, [seiten_inhalt_html_liste,\n",
    "                        seiten_inhalt_liste,\n",
    "                        URL_Liste,\n",
    "                        Datum_Liste]))) > 1:\n",
    "        tools.schreibe_log_file(scraper_name, 'Listen sind verschieden lang')\n",
    "        tools.schreibe_e_mail(scraper_name, f'Listen sind nicht gleich lang  -- {datetime.datetime.now()} --')\n",
    "        return\n",
    "    else:\n",
    "        # DataFrame erstellen\n",
    "        df = pd.DataFrame({\"seite\": \"stepstone\",\n",
    "                        \"seiten_inhalt_html\": seiten_inhalt_html_liste,\n",
    "                        \"seiten_inhalt\": seiten_inhalt_liste, \n",
    "                        \"url\": URL_Liste, \n",
    "                        \"datum\":Datum_Liste,\n",
    "                        \"storno\": False})\n",
    "        \n",
    "    ## Daten in die Datenbank einfügen\n",
    "    # Schreibe den bereinigten DataFrame in die Datenbank\n",
    "    try:\n",
    "        df.to_sql(name=tabelle_rohdaten.split('.')[1],\n",
    "                schema=tabelle_rohdaten.split('.')[0],\n",
    "                con=con, if_exists='append', index=False, )\n",
    "        tools.schreibe_log_file(scraper_name, f'Es wurden {len(df)} Daten von Stepstone hinzugefügt')\n",
    "    except:\n",
    "        tools.schreibe_log_file(scraper_name, 'Datenbank konnte nicht gefüllt werden => Dataframe fehlt')\n",
    "        tools.schreibe_e_mail(scraper_name, f'Datenbank konnte nicht gefüllt werden => Dataframe fehlt  -- {datetime.datetime.now()} --')\n",
    "\n",
    "    # Verbindung zur SQL-Datenbank schließen\n",
    "    con.dispose()\n",
    "print(\"fertig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stepstone_scraper(\"Data Analyst\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                                      Jobtitel\n0                                 Data Analyst\n1                          Junior Data Analyst\n2   Data Scientist & Machine Learning Engineer\n3                                 Data Expert \n4                     Data Warehouse Developer\n5                        (Junior) Data Analyst\n6                               Data Scientist\n7             Customer Data Analyst / Engineer\n8                            Big Data Engineer\n9                              Data Consultant\n10                     Master Data Coordinator\n11     Specialist Data Analytics & Master Data\n12                             Data Enthusiast",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Jobtitel</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Data Analyst</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Junior Data Analyst</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Data Scientist &amp; Machine Learning Engineer</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Data Expert</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Data Warehouse Developer</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>(Junior) Data Analyst</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Data Scientist</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Customer Data Analyst / Engineer</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Big Data Engineer</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Data Consultant</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Master Data Coordinator</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Specialist Data Analytics &amp; Master Data</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Data Enthusiast</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ich arbeite mit zwei Computern, und leider hat die Datei für die Suchbegriffe (Jobtitel) jeweils ein anderen Speicherort\n",
    "try:\n",
    "    suchbegriffe = pd.read_excel(r\"C:\\Users\\Admin\\DreamJobs\\Benötigte Dateien\\Jobtitel.xlsx\",\n",
    "                                sheet_name='Stepstone')\n",
    "except:\n",
    "    suchbegriffe = pd.read_excel(r\"C:\\Users\\Admin\\DreamJobs\\Benötigte Dateien\\Jobtitel.xlsx\",\n",
    "                                sheet_name='Stepstone')  \n",
    "display(suchbegriffe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 Jobtitel gefunden\n",
      "0: Suche nach \"Data Analyst\", Start: 08:53:32 Uhr\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'psycopg2'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_2828\\717796547.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mjobtitel\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msuchbegriffe\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mJobtitel\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf'{i}: Suche nach \"{jobtitel}\", Start: {datetime.datetime.now().strftime(\"%H:%M:%S\")} Uhr'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m     \u001B[0mstepstone_scraper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mjobtitel\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"fertig\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_2828\\2708348042.py\u001B[0m in \u001B[0;36mstepstone_scraper\u001B[1;34m(jobtitel, suchort)\u001B[0m\n\u001B[0;32m     32\u001B[0m     \u001B[0mtools\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mschreibe_log_file\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mscraper_name\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34mf'suche nach {jobtitel}'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 34\u001B[1;33m     \u001B[0mcon\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtools\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconnect_db\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     35\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     36\u001B[0m     \u001B[1;31m# ChromeOptions erstellen\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\DreamJobs\\Skripte\\scraper\\alt\\tools.py\u001B[0m in \u001B[0;36mconnect_db\u001B[1;34m(abschnitt)\u001B[0m\n\u001B[0;32m     73\u001B[0m     \u001B[0mdb_port\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mabschnitt\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"PORT\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     74\u001B[0m     \u001B[0ms\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34mf\"postgresql://{db_user}:{db_pw}@{db_host}:{db_port}/{db_name}\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 75\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0msqlalchemy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcreate_engine\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     76\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     77\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<string>\u001B[0m in \u001B[0;36mcreate_engine\u001B[1;34m(url, **kwargs)\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\util\\deprecations.py\u001B[0m in \u001B[0;36mwarned\u001B[1;34m(fn, *args, **kwargs)\u001B[0m\n\u001B[0;32m    307\u001B[0m                         \u001B[0mstacklevel\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    308\u001B[0m                     )\n\u001B[1;32m--> 309\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    310\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    311\u001B[0m         \u001B[0mdoc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__doc__\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__doc__\u001B[0m \u001B[1;32mor\u001B[0m \u001B[1;34m\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\create.py\u001B[0m in \u001B[0;36mcreate_engine\u001B[1;34m(url, **kwargs)\u001B[0m\n\u001B[0;32m    558\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mk\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    559\u001B[0m                 \u001B[0mdbapi_args\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mk\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpop_kwarg\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mk\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 560\u001B[1;33m         \u001B[0mdbapi\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdialect_cls\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdbapi\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m**\u001B[0m\u001B[0mdbapi_args\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    561\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    562\u001B[0m     \u001B[0mdialect_args\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"dbapi\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdbapi\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\dialects\\postgresql\\psycopg2.py\u001B[0m in \u001B[0;36mdbapi\u001B[1;34m(cls)\u001B[0m\n\u001B[0;32m    780\u001B[0m     \u001B[1;33m@\u001B[0m\u001B[0mclassmethod\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    781\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mdbapi\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcls\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 782\u001B[1;33m         \u001B[1;32mimport\u001B[0m \u001B[0mpsycopg2\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    783\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    784\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mpsycopg2\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'psycopg2'"
     ]
    }
   ],
   "source": [
    "print(f'{len(suchbegriffe)} Jobtitel gefunden')\n",
    "for i, jobtitel in enumerate(suchbegriffe.Jobtitel):\n",
    "    print(f'{i}: Suche nach \"{jobtitel}\", Start: {datetime.datetime.now().strftime(\"%H:%M:%S\")} Uhr')\n",
    "    stepstone_scraper(jobtitel)\n",
    "print(\"\")\n",
    "print(\"fertig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
